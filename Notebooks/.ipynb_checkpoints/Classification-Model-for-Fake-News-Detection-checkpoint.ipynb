{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "342f34ac",
   "metadata": {},
   "source": [
    "# Fake News Detector Machine Learning Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ad8ce4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'sisco' from 'scripts' (/Users/jorgesisco/Documents/GitHub/ml-fake-news/Fake-News-Detection-Machine-Learning-Pipeline/Notebooks/scripts.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j2/64m37fcx0fx_w8wpmhz91lmh0000gn/T/ipykernel_28833/1775317869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Importing Function from script.py file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msisco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'sisco' from 'scripts' (/Users/jorgesisco/Documents/GitHub/ml-fake-news/Fake-News-Detection-Machine-Learning-Pipeline/Notebooks/scripts.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# Importing Function from script.py file\n",
    "from scripts import word_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782460c3",
   "metadata": {},
   "source": [
    "# Loading The Datasets\n",
    "\n",
    "For this exploratory Pipeline, I am using two separated datasets I found on Google, one CSV file is the **true news dataset** and the other file is the ****fake news dataset****."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd765b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('Datasets/Public_Data/corona_fake.csv')\n",
    "df_fake = df_fake.drop(['text', 'source', 'label'], axis=1)\n",
    "df_fake.rename(columns = {'title':'Tweets'}, inplace = True)\n",
    "df_fake = df_fake.dropna()\n",
    "print(df_fake.shape)\n",
    "df_fake.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c471c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.read_csv('Datasets/Old_Collected_Data/COVID-19-Truth.csv')\n",
    "df_true = df_true.drop(['Tweet URL', 'Tweet ID', 'User ID', 'Unnamed: 0'], axis=1)\n",
    "print(df_true.shape)\n",
    "df_true.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cd9d4-c690-4b7e-b64d-18b9e2bfdc20",
   "metadata": {},
   "source": [
    "# Removing Empty Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f70bed-4a22-4209-84e9-41db80abda15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e523bc-8ec0-4f02-87a5-b53d281a29d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4007799-8d31-459e-aec1-ff9fd1348f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fdd163-9ffa-4482-a132-d72b190db1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21b52b0c",
   "metadata": {},
   "source": [
    "# Adding Lables to each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake[\"class\"] = 0\n",
    "df_true[\"class\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd66ae2",
   "metadata": {},
   "source": [
    "# Dataset Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5454d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.shape, df_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01177f15",
   "metadata": {},
   "source": [
    "# Taking some rows from the two datasets\n",
    "\n",
    "I am creating two variables to store the last 10 rows of each dataset to then export it to a new csv file.\n",
    "\n",
    "Our Goal is to use those news for manual testing after we build Our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cfa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing last 10 rows of df_fake into a variable\n",
    "df_fake_manual_testing = df_fake.tail(10)\n",
    "\n",
    "# Deleting the last 10 rows from dataset\n",
    "df_fake.drop(df_fake.tail(10).index,\n",
    "        inplace = True)\n",
    "\n",
    "# Storing last 10 rows of df_true into a variable\n",
    "df_true_manual_testing = df_true.tail(10)\n",
    "\n",
    "# Deleting the last 10 rows from dataset\n",
    "df_true.drop(df_true.tail(10).index,\n",
    "        inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8c698",
   "metadata": {},
   "source": [
    "# Dataset Shapes Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fake.shape, df_true.shape\n",
    "df_true_manual_testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da1b8d",
   "metadata": {},
   "source": [
    "# Creating a Dataframe to store the 10 rows from df_true and df_fake\n",
    "\n",
    "Also I am exporting that datafram in an CSV file out of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01362add",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_testing = pd.concat([df_fake_manual_testing, df_true_manual_testing], axis=0)\n",
    "df_manual_testing = df_manual_testing.sample(frac=1)\n",
    "df_manual_testing[\"Tweets\"] = df_manual_testing[\"Tweets\"].apply(word_drop)\n",
    "df_manual_testing.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d111461-1b8c-4462-8e27-910db80f8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_testing[\"Tweets\"] = df_manual_testing[\"Tweets\"].apply(word_drop)\n",
    "df_manual_testing.to_csv(\"Datasets/manual_testing.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20280408-9a29-4e2c-9762-1077892356e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "396e66c7",
   "metadata": {},
   "source": [
    "# Feature Engineering \n",
    "## Mergin the Datasets into One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.concat([df_fake, df_true], axis=0)\n",
    "df_merge.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93fc73",
   "metadata": {},
   "source": [
    "## Deleting Titlte, subject and date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_merge.drop(['title', 'subject', 'date'], axis=1)\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bb59d",
   "metadata": {},
   "source": [
    "## Randomizing rows in the Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merge.sample(frac=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b835e",
   "metadata": {},
   "source": [
    "# Checking for Null values\n",
    "\n",
    "No null values as you can see in the results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff076507",
   "metadata": {},
   "source": [
    "# Creating a function that will take the text from the dataset and remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945bc497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word_drop(text):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub('\\[.*?\\]', '', text)\n",
    "#     text = re.sub('\\\\W', \" \", text)\n",
    "#     text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "#     text = re.sub('<.*?>+', '', text)\n",
    "#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "#     text = re.sub('\\n', '', text)\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05992cfa",
   "metadata": {},
   "source": [
    "## Calling funtion on the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweets\"] = df[\"Tweets\"].apply(word_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd8444",
   "metadata": {},
   "source": [
    "# Defining our Dependent and Independent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['Tweets']\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd6ff4",
   "metadata": {},
   "source": [
    "# Spliting the Data in Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fcaad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= .25)\n",
    "\n",
    "print('df', df.shape)\n",
    "print('x_train', x_train.shape)\n",
    "print('x_test', x_test.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464f24a",
   "metadata": {},
   "source": [
    "# Converting Train data text into Vector\n",
    "Using sklearn.feature_extraction.text.TfidfVectorizer\n",
    "// from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a714941",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization = TfidfVectorizer()\n",
    "x_train_vectors = vectorization.fit_transform(x_train)\n",
    "x_test_vectors = vectorization.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09448eb",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "## Training Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b3660",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression()\n",
    "model_1.fit(x_train_vectors, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0722d",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model_1.predict(x_test_vectors)\n",
    "accuracy = accuracy_score(y_test, y_predicted)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_predicted) \n",
    "\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print('F1-Score', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16bbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccca51-ea60-4b6d-b00f-604f001edf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('Datasets/manual_testing.csv')\n",
    "df_2 = df_2.drop(['Unnamed: 0'], axis=1)\n",
    "print(df_2.shape)\n",
    "df_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08b605-01d8-4a2d-b291-81b3351cd3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a5981-1a4d-4844-ba82-94a06571544b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5d016-6a0a-4b1b-8943-af023bb81d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b9ac5-4677-41b4-a3cf-a0775cecb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = manual_Testing\n",
    "# input_\n",
    "input_trans = vectorization.transform(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb24c7e-33a2-4a7b-87ea-1fe61188e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vectorization.transform(df.loc[0, ['Tweets']]).todense()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109eb939-8b27-4352-82a5-59d77f55d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a51da8-3f06-460c-adf6-f9303b2b4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model_1.predict(np.asarray(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c6061-7a33-4191-8134-69dcf0aab196",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26329b-6c3f-4f36-8dcf-68e842d7326a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8784d6-8706-481e-9ef6-69dc2c60b46c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Fake-News-Pipeline",
   "language": "python",
   "name": "ml-fake-news-pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
