{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "342f34ac",
   "metadata": {},
   "source": [
    "# Fake News Detector Machine Learning Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing Function from script.py file\n",
    "from scripts import word_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782460c3",
   "metadata": {},
   "source": [
    "# Loading The Datasets\n",
    "\n",
    "For this exploratory Pipeline, I am using two separated datasets I found on Google, one CSV file is the **true news dataset** and the other file is the ****fake news dataset****."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd765b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('Datasets/Public_Data/corona_fake.csv')\n",
    "df_fake = df_fake.drop(['text', 'source', 'label'], axis=1)\n",
    "df_fake.rename(columns = {'title':'Tweets'}, inplace = True)\n",
    "df_fake = df_fake.dropna()\n",
    "print(df_fake.shape)\n",
    "df_fake.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c471c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.read_csv('Datasets/Old_Collected_Data/COVID-19-Truth.csv')\n",
    "df_true = df_true.drop(['Tweet URL', 'Tweet ID', 'User ID', 'Unnamed: 0'], axis=1)\n",
    "print(df_true.shape)\n",
    "df_true.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b52b0c",
   "metadata": {},
   "source": [
    "# Adding Labels to each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake[\"class\"] = 0\n",
    "df_true[\"class\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd66ae2",
   "metadata": {},
   "source": [
    "# Dataset Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5454d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.shape, df_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b258ed-5bdb-442b-a2ae-96e6de739117",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_n = abs(df_fake.shape[0]-df_true.shape[0])\n",
    "\n",
    "if df_fake.shape[0] > df_true.shape[0]:\n",
    "    drop_indices = np.random.choice(df_fake.index, remove_n, replace=False)\n",
    "    df_fake = df_fake.drop(drop_indices)\n",
    "    \n",
    "    \n",
    "elif df_true.shape[0] > df_fake.shape[0]:\n",
    "    drop_indices = np.random.choice(df_true.index, remove_n, replace=False)\n",
    "    df_true = df_true.drop(drop_indices)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01177f15",
   "metadata": {},
   "source": [
    "# Taking some rows from the two datasets\n",
    "\n",
    "I am creating two variables to store the last 10 rows of each dataset to then export it to a new csv file.\n",
    "\n",
    "Our Goal is to use those news for manual testing after we build Our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cfa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing last 10 rows of df_fake into a variable\n",
    "df_fake_manual_testing = df_fake.tail(10)\n",
    "\n",
    "# Deleting the last 10 rows from dataset\n",
    "df_fake.drop(df_fake.tail(10).index,\n",
    "        inplace = True)\n",
    "\n",
    "# Storing last 10 rows of df_true into a variable\n",
    "df_true_manual_testing = df_true.tail(10)\n",
    "\n",
    "# Deleting the last 10 rows from dataset\n",
    "df_true.drop(df_true.tail(10).index,\n",
    "        inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8c698",
   "metadata": {},
   "source": [
    "# Dataset Shapes Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fake.shape, df_true.shape\n",
    "df_true_manual_testing.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da1b8d",
   "metadata": {},
   "source": [
    "# Creating a Dataframe to store the 10 rows from df_true and df_fake\n",
    "\n",
    "Also I am exporting that datafram in an CSV file out of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01362add",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_testing = pd.concat([df_fake_manual_testing, df_true_manual_testing], axis=0)\n",
    "df_manual_testing = df_manual_testing.sample(frac=1)\n",
    "df_manual_testing[\"Tweets\"] = df_manual_testing[\"Tweets\"].apply(word_drop)\n",
    "df_manual_testing.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d111461-1b8c-4462-8e27-910db80f8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_testing.to_csv(\"Datasets/manual_testing.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e66c7",
   "metadata": {},
   "source": [
    "# Feature Engineering \n",
    "## Mergin the Datasets into One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.concat([df_fake, df_true], axis=0)\n",
    "df = df_merge.sample(frac=1)\n",
    "df_merge.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b835e",
   "metadata": {},
   "source": [
    "# Checking for Null values\n",
    "\n",
    "No null values as you can see in the results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # Alles Gut hier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05992cfa",
   "metadata": {},
   "source": [
    "## Calling funtion on the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweets\"] = df[\"Tweets\"].apply(word_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae3593-ea0b-402e-b51b-1a5917c540fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Datasets/df.csv\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Fake-News-Pipeline",
   "language": "python",
   "name": "ml-fake-news-pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
