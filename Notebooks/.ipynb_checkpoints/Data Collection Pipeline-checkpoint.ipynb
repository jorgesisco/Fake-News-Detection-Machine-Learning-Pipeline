{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480db4cf-c314-4428-93d4-7547fa7bc8ce",
   "metadata": {},
   "source": [
    "# Collecting Textual News Data from Twitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462a4af-0549-4065-931f-c0b38d8e5685",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91b13a-5abe-4462-b404-bfc0ccbfd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tweepy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e209c-db8c-4996-b850-f63a35a83e28",
   "metadata": {},
   "source": [
    "## Setting Up Twitter API and Tokens from .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d6419-0938-40a8-a19f-503049d37f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d93073-d4b5-4d24-a44b-f78233ed5ee3",
   "metadata": {},
   "source": [
    "## Setting Up Twitter Screaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d0056-3217-437d-a946-d0fb35c74d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "userID = []\n",
    "tweetID = []\n",
    "tweet_url = []\n",
    "username = []\n",
    "next_page_token = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f71e00-0fae-4057-be29-5e158be242a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClient():\n",
    "    \n",
    "    client = tweepy.Client(bearer_token= os.getenv(\"BEARER_TOKEN\"),\n",
    "                           consumer_key=os.getenv(\"API_KEY\"), \n",
    "                           consumer_secret=os.getenv(\"API_KEY_SECRET\"), \n",
    "                           access_token=os.getenv(\"ACCESS_TOKEN\"),\n",
    "                           access_token_secret=os.getenv(\"ACCESS_TOKEN_SECRET\"))\n",
    "\n",
    "    return client\n",
    "\n",
    "\n",
    "def searchTweets(query, qty):\n",
    "    \n",
    "    client = getClient()\n",
    "    client_call = client.search_all_tweets(query=query, max_results=qty, expansions='author_id')\n",
    "    \n",
    "    if not client_call.data is None and len(client_call.data) > 0: #Checking if there are results or not before looping...\n",
    "        for tweet in client_call.data:\n",
    "            tweets.append(tweet.text)\n",
    "            id_ = tweet.id\n",
    "            tweetID.append(id_)\n",
    "            user_id = tweet.author_id\n",
    "            userID.append(user_id)\n",
    "            tweet_url.append('https://twitter.com/{}/status/{}'.format(user_id, id_))\n",
    "            \n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        next_page = client_call.meta['next_token']\n",
    "        next_page_token.append(next_page)\n",
    "    except KeyError:\n",
    "        print(\"NO NEXT TOKEN AVAILABLE, THIS MEANS YOU GOT ONLY ONE PAGE.... :( \")\n",
    "\n",
    "    df = pd.DataFrame({'Tweets': tweets, \"Tweet URL\": tweet_url, 'Tweet ID': tweetID, \"User ID\": userID})\n",
    "    # df = df[~df.Tweets.str.contains(\"RT\")]\n",
    "           \n",
    "    return df \n",
    "\n",
    "\n",
    "def search_tweets_next(query, qty, next_page_token, pagination):\n",
    "    \n",
    "    client = getClient()\n",
    "\n",
    "    client_call_ = client.search_all_tweets(query=query, max_results=qty, expansions='author_id', next_token=next_page_token[pagination])\n",
    "    #If you want to keep collecting, change to 20 the list above\n",
    "    if not client_call_.data is None and len(client_call_.data) > 0: #Checking if there are results or not before looping...\n",
    "        for tweet in client_call_.data:\n",
    "            tweets.append(tweet.text)\n",
    "            id_ = tweet.id\n",
    "            tweetID.append(id_)\n",
    "            user_id = tweet.author_id\n",
    "            userID.append(user_id)   \n",
    "            tweet_url.append('https://twitter.com/{}/status/{}'.format(user_id, id_))\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        next_page = client_call.meta['next_token']\n",
    "        next_page_token.append(next_page)\n",
    "    except KeyError:\n",
    "        print(\"NO NEXT TOKEN AVAILABLE, THIS MEANS YOU GOT ONLY ONE PAGE.... :( \")\n",
    "    \n",
    "    df = pd.DataFrame({'Tweets': tweets, \"Tweet URL\": tweet_url, 'Tweet ID': tweetID, \"User ID\": userID})\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08be469-a464-4379-bb3a-d98c4283522a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Calling Function to search for tweets\n",
    "This function will get all tweets in a dataframe and then it will be exported on a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bcf583-2d69-4850-a4e7-02401e684079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Data Request\n",
    "df_1 = searchTweets('from:BBCWorld covid-19 -is:retweet lang:en', 500)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24ee49-6edc-4e95-bd32-69ae4bc679a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data requests adding pagination token (each request gets 500 tweets, every single execuion I have to change the token manually)\n",
    "pagination = 0\n",
    "df_ = search_tweets_next('covid-19 news -is:retweet lang:en', 500, next_page_token, pagination)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beef963-e79c-4a4f-842a-adfcb6c98716",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_1, df_2, df_3]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddb305-a29d-4d55-802f-8bc00cf15e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c89be-5907-47f5-a5de-e41dd40ed951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11ebb5-32f0-4798-8f9c-f9a2d56f43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "userID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608577c0-6564-44df-8f1d-6d3daa5330ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('Collected_Data/COVID-19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c6a2c-341c-4849-936c-0bf34c88f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(next_page_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4442a0-150f-4b9b-84cd-b96d92ef4ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca80bdf-b71b-486a-9c85-4c15e1c1bb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Fake-News-Pipeline",
   "language": "python",
   "name": "ml-fake-news-pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
