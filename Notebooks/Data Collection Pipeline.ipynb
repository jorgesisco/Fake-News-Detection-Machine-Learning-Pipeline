{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480db4cf-c314-4428-93d4-7547fa7bc8ce",
   "metadata": {},
   "source": [
    "# Collecting Textual News Data from Twitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462a4af-0549-4065-931f-c0b38d8e5685",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa91b13a-5abe-4462-b404-bfc0ccbfd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tweepy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e209c-db8c-4996-b850-f63a35a83e28",
   "metadata": {},
   "source": [
    "## Setting Up Twitter API and Tokens from .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776d6419-0938-40a8-a19f-503049d37f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d93073-d4b5-4d24-a44b-f78233ed5ee3",
   "metadata": {},
   "source": [
    "## Setting Up Twitter Screaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517d0056-3217-437d-a946-d0fb35c74d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "userID = []\n",
    "tweetID = []\n",
    "tweet_url = []\n",
    "username = []\n",
    "next_page_token = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f71e00-0fae-4057-be29-5e158be242a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClient():\n",
    "    \n",
    "    client = tweepy.Client(bearer_token= os.getenv(\"BEARER_TOKEN\"),\n",
    "                           consumer_key=os.getenv(\"API_KEY\"), \n",
    "                           consumer_secret=os.getenv(\"API_KEY_SECRET\"), \n",
    "                           access_token=os.getenv(\"ACCESS_TOKEN\"),\n",
    "                           access_token_secret=os.getenv(\"ACCESS_TOKEN_SECRET\"))\n",
    "\n",
    "    return client\n",
    "\n",
    "\n",
    "def searchTweets(query, qty):\n",
    "    \n",
    "    client = getClient()\n",
    "    client_call = client.search_all_tweets(query=query, max_results=qty, expansions='author_id')\n",
    "    \n",
    "    if not client_call.data is None and len(client_call.data) > 0: #Checking if there are results or not before looping...\n",
    "        for tweet in client_call.data:\n",
    "            tweets.append(tweet.text)\n",
    "            id_ = tweet.id\n",
    "            tweetID.append(id_)\n",
    "            user_id = tweet.author_id\n",
    "            userID.append(user_id)\n",
    "            tweet_url.append('https://twitter.com/{}/status/{}'.format(user_id, id_))\n",
    "            \n",
    "    else:\n",
    "        return ['NOOOOO DATA']\n",
    "    \n",
    "    try:\n",
    "        next_page = client_call.meta['next_token']\n",
    "        next_page_token.append(next_page)\n",
    "    except KeyError:\n",
    "        print(\"NO NEXT TOKEN AVAILABLE, THIS MEANS YOU GOT ONLY ONE PAGE.... :( \")\n",
    "\n",
    "    df = pd.DataFrame({'Tweets': tweets, \"Tweet URL\": tweet_url, 'Tweet ID': tweetID, \"User ID\": userID})\n",
    "           \n",
    "    return df \n",
    "\n",
    "\n",
    "def search_tweets_next(query, qty, next_page_token, pagination):\n",
    "    \n",
    "    client = getClient()\n",
    "\n",
    "    client_call_ = client.search_all_tweets(query=query, max_results=qty, expansions='author_id', next_token=next_page_token[pagination])\n",
    "   \n",
    "    if not client_call_.data is None and len(client_call_.data) > 0:\n",
    "        for tweet in client_call_.data:\n",
    "            tweets.append(tweet.text)\n",
    "            id_ = tweet.id\n",
    "            tweetID.append(id_)\n",
    "            user_id = tweet.author_id\n",
    "            userID.append(user_id)   \n",
    "            tweet_url.append('https://twitter.com/{}/status/{}'.format(user_id, id_))\n",
    "    else:\n",
    "        return ['NOOOOO DATA']\n",
    "    \n",
    "    try:\n",
    "        next_page = client_call_.meta['next_token']\n",
    "        next_page_token.append(next_page)\n",
    "    except KeyError:\n",
    "        print(\"NO NEXT TOKEN AVAILABLE, THIS MEANS YOU GOT UNTIL THIS PAGE:( .... :( \")\n",
    "    \n",
    "    df = pd.DataFrame({'Tweets': tweets, \"Tweet URL\": tweet_url, 'Tweet ID': tweetID, \"User ID\": userID})\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_user_by_id(userID):\n",
    "    \n",
    "    client = getClient()\n",
    "    client_call = client.get_user(id=userID)\n",
    "    user_id = client_call.data.username\n",
    "    \n",
    "    return user_id\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08be469-a464-4379-bb3a-d98c4283522a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Calling Function to search for tweets\n",
    "This function will get all tweets in a dataframe and then it will be exported on a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0bcf583-2d69-4850-a4e7-02401e684079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Data Request\n",
    "NewsAccounts = ['factcheckdotorg', 'CNN', 'Forbes', 'BBCBreaking', 'BBC', \n",
    "                'nytimes', 'WSJ', 'washingtonpost', 'NewYorker', \n",
    "                'AP', 'Reuters', 'Bloomberg', 'ForeignAffairs', \n",
    "                'TheAtlantic', 'politico']\n",
    "\n",
    "FakeNewsAccounts = ['TheOnion']\n",
    "\n",
    "df_t = searchTweets('from:{} -is:retweet lang:en'.format(NewsAccounts[0]), 500)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "133105a6-2177-443f-a49e-f43c65853425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('Datasets/TrustedNews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1575c0e4-aeb8-4aa0-88b6-0a405359ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = pd.concat([df, df_], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ffa946-330a-4743-a727-1bb7ae9c96c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33295, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc24ee49-6edc-4e95-bd32-69ae4bc679a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO NEXT TOKEN AVAILABLE, THIS MEANS YOU GOT UNTIL THIS PAGE:( .... :( \n"
     ]
    }
   ],
   "source": [
    "#Data requests adding pagination token (each request gets 500 tweets, every single execuion I have to change the token manually)\n",
    "pagination = 0\n",
    "df_ = search_tweets_next('from:{} -is:retweet lang:en'.format(NewsAccounts[0]), 500, next_page_token, pagination)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de86b674-73fb-436c-82b4-77d53fb1c6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(695, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de259f-6450-4e4f-932c-6803847c0772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beef963-e79c-4a4f-842a-adfcb6c98716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf89ab-3962-4a7e-860a-39bf79e60554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddb305-a29d-4d55-802f-8bc00cf15e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c89be-5907-47f5-a5de-e41dd40ed951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11ebb5-32f0-4798-8f9c-f9a2d56f43d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "608577c0-6564-44df-8f1d-6d3daa5330ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .to_csv('Datasets/TrustedNews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b54619-b28e-468b-b4ef-d00ef7498e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad9edeca-ee81-46d5-ba84-ef6e58e43eb2",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4442a0-150f-4b9b-84cd-b96d92ef4ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca80bdf-b71b-486a-9c85-4c15e1c1bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_75.drop(df_75.index[df_75['User ID'] == 428333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06c45888-dd38-49f8-8f40-9da13050b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Datasets/TrustedNews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "920a228a-dd5c-4d7c-b6da-70f7afcf98de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32600, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f1fdf55-6b79-4758-8104-78476c617ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33295, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20608a-2ccc-40c5-901d-dbe1bd384f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Fake-News-Pipeline",
   "language": "python",
   "name": "ml-fake-news-pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
